This systematic review was conducted by two reviewers from different institutions following the procedures as described in \cite{kitchenham:2004, Moher:2009}. 
A systematic review can be conducted for several reasons such as (a) the summarisation and comparison, in terms of advantages and disadvantages, of various approaches in a field, (b) the identification of open problems, (c) the contribution of a joint conceptualization comprising the various approaches developed in a field, or (d) the synthesis of a new idea to cover the emphasized problems. 
This systematic review comprises of all the above mentioned reasons, in that, it summarises and compares various data quality assessment methodologies as well as identifies open problems focused on Linked Open Data.
Moreover, it contributes a conceptualization of the data quality assessment field and thereafter proposes a new method for data quality assessment for LOD.

\paragraph{Related surveys.}
In order to justify the need of conducting the systematic review, we first conducted a search for related surveys and literature reviews.
We did not come across any study focused on data quality assessment methodologies and tools for Linked Data. 
However, there is a comprehensive review \cite{Batini:2006}, which surveys 12 methodologies for assessing the data quality of datasets available on the web in structured or semi-structured formats.
% what's the difference: we focus on structured data, the Linked Data field only emerged after publishing of the survey, hence, certain crucial aspects such as ... are not covered.
%LOD vs databases (information systems)
%we looked at approaches of assessing linked data 
%he focused only on dimensions
%we introduced new dimensions, metrics
%we introduced assessment steps specifically for LOD as we didn't find a standardised methodology in the approaches we came across

\paragraph{Research question.}
The goal of this review is to analyse existing methodologies for assessing the quality of structured data, with particular interest in Linked Data.
To achieve this goal, we aim to answer the following general research question: \\


\emph{What are the existing approaches for assessing the quality of Linked Data datasets?}\\

We can divide this general research question into further fine-grained research questions such as: 
\begin{itemize}
\item		\textit{What are the problems that each approach assesses?}\\
\item		\textit{Which are the quality dimensions and metrics supported by the proposed approaches?}\\
\item		\textit{What are the assessment methods proposed by the different approaches?}
\end{itemize}

\paragraph{Define eligibility criteria.}
The eligibility criteria is an important element of any systematic review. 
First, each member created a set of inclusion and exclusion criteria on their own. 
Second, as a result of a discussion between both members a list of eligible criteria was obtained as follows:
\begin{itemize}
\item Inclusion criteria:
\begin{itemize}
\item Studies that were published in English between 2002 and 2012.
\item Studies that were focused on data quality assessment in Linked Data
\end{itemize}
\item Exclusion criteria:
\begin{itemize}
\item Studies that were focused on data quality management
\item Studies that did not focus neither on Linked Data nor on structured data
\item Studies that did non propose any methodology or framework about the assessment of quality in Linked Data
%\item Studies that did not focus on quality assessment of Linked Data but only mentions the term
\end{itemize}
\end{itemize}

\begin{figure*}[ht]
\includegraphics[width=6.5in]{Numberofarticles.pdf}
\caption{Number of articles retrieved during literature search.}
\label{fig:noofarticles}
\end{figure*}

\paragraph{Search strategy.}
Search strategies in a systematic review are usually iterative and are ran separately by both members. 
Based on the research question and the eligible criteria, each reviewer identified several terms that were most appropriate for this systematic review, such as: \textit{data}, \textit{quality}, \textit{data quality}, \textit{assessment}, \textit{evaluation}, \textit{methodology}, \textit{improvement}, or \textit{linked data}, which were used as follows:
\begin{itemize}
\item \textit{linked data} and (\textit{quality} OR \textit{assessment} OR \textit{evaluation} OR \textit{methodology} OR \textit{improvement})
\item  \textit{data} OR \textit{quality} OR \textit{data quality} AND \textit{assessment} OR \textit{evaluation} OR \textit{methodology} OR \textit{improvement}
\end{itemize}
%The next decision was to find the suitable field (i.e. title, abstract and full-text) to apply the search
string on. 
In our experience, searching in the \textit{title} alone does not always provide us with all relevant publications. 
Thus, \textit{abstract} or \textit{full-text} of publications should also potentially be included. 
On the other hand, since the search on the full-text of studies results in many irrelevant publications, we chose to apply the search query first on the \textit{title} and \textit{abstract} of the studies.
This means a study is selected as a candidate study if its \textit{title} or \textit{abstract} contains the keywords defined in the search string.

After we defined the search strategy, we applied the keyword search in the following list of search engines, digital libraries, journals, conferences and their respective workshops: \\
Search Engines and digital libraries:
\begin{itemize}
\item Google Scholar
\item ISI Web of Science
\item ACM Digital Library
\item IEEE Xplore Digital Library
\item Springer Link
\item Science Direct
\end{itemize}
Journals:
\begin{itemize}
\item Semantic Web Journal
\item Journal of Web Semantics
\item Journal of Data and Information Quality
\item Journal of Data and Knowledge Engineering
\end{itemize}
Conferences and their Respective Workshops:
\begin{itemize}
\item International Semantic Web Conference (ISWC)
\item European Semantic Web Conference (ESWC)
\item Asian Semantic Web Conference (ASWC)
\item International World Wide Web Conference (WWW)
\item Semantic Web in Provenance Management (SWPM)
\item Consuming Linked Data (COLD)
\item Linked Data on the Web (LDOW)
\item Web Quality
\end{itemize}
Thereafter the bibliographic metadata about the 118 potentially relevant primary study were recorded using the bibliography management platform Mendeley\footnote{https://www.mendeley.com/}.

\paragraph{Titles and abstract reviewing.}
Both reviewers independently screened the titles and abstracts of the retrieved 118 articles to identify the potentially eligible articles. 
In case of disagreement while merging the lists, the problem was resolved either by mutual consensus or by creating a list of articles to go under a more detailed review. 
Then, both the reviewers compared the articles and based on mutual agreement obtained a final list of 64 articles to be included. 

\paragraph{Retrieving further potential articles.}
In order to ensure that all relevant articles were included, an additional strategy was applied such as:
\begin{itemize}
\item Looking up the reference in the selected articles
\item Looking up the article title in Google Scholar and retrieving the "Cited By" papers to check against the eligibility criteria
\item Taking each data quality dimension individually and perform a related article search
\end{itemize}
After performing these search strategies, we further retrieved 4 additional articles. 

\paragraph{Extracting data for quantitative and qualitative analysis.}
An overview of the search methodology and the number of retrieved articles at each step is shown in Figure \ref{fig:noofarticles}.
The result of the above described methodology is 20 papers from 2002 to 2012 that are reported in Table~\ref{selectedpapers} which are the core of our survey.
The next step was then to extract data from each of the articles to perform quantitative and qualitative analysis.

%\textit{To analyze the information appropriately, we required a suitable qualitative data analysis method applicable to our dataset. We used coding as our qualitative analysis methods}

\begin{table*}[htb]
\caption{List of the selected papers.} 
\label{selectedpapers}
\begin{tabular}{ | p{2.7cm} | p{10cm} | }
\hline
\textbf{Citation} & \textbf{Title} \\
\hline
Gil et.al., 2002 & Trusting Information Sources One Citizen at a Time \\
\hline
Golbeck et. al., 2003 & Trust Networks on the Semantic Web \\
\hline
Mostafavi et.al., 2004 & An ontology-based method for quality assessment of spatial data bases\\
\hline
Golbeck, 2006 & Using Trust and Provenance for Content Filtering on the Semantic Web \\
\hline
Gil et.al., 2007 & Towards content trust of web resources \\
\hline
Lei et.al., 2007 & A framework for evaluating semantic metadata \\
\hline
Hartig, 2008 & Trustworthiness of Data on the Web \\
\hline
Bizer et.al.,2009 & Quality-driven information filtering using the WIQA policy framework \\	
\hline
B\"ohm et.al., 2010 & Profiling linked open data with ProLOD \\	
\hline
Chen et.al., 2010 & Hypothesis generation and data quality assessment through association mining \\
\hline
Flemming et.al., 2010 & Assessing the quality of a Linked Data source \\
\hline
Hogan et.al., 2010 & Weaving the Pedantic Web \\
\hline
Shekarpour et.al., 2010 & Modeling and evaluation of trust with an extension in semantic web \\
\hline
Gamble et.al., 2011 & Quality, Trust, and Utility of Scientific Data on the Web: Towards a Joint Model \\
\hline
Jacobi et.al., 2011 & Rule-Based Trust Assessment on the Semantic Web \\
\hline
Bonatti et. al., 2012 & Robust and scalable linked data reasoning incorporating provenance and trust annotations \\
\hline
Gu\'eret et. al., 2012 & Assessing Linked Data Mappings Using Network Measures \\
\hline
Hogan et.al.,2012 & An empirical survey of Linked Data conformance \\
\hline
Maurino et.al., 2012 & Capturing the Age of Linked Open Data: Towards a Dataset-independent Framework \\
\hline
Mendes et.al.,2012 & Sieve: Linked Data Quality Assessment and Fusion \\
\hline
\end{tabular}
\end{table*}

\paragraph{Comparison perspective of selected approaches.}
There exist several perspectives that can be used to analyze and compare the selected approaches, such as:
\begin{itemize}
\item the definitions of the core concepts
\item the dimensions and metrics proposed by each approach
\item the type of data that is considered for the assessment
\item the level of automatization of supported tools
\item the phases/steps that compose the assessment methods
\end{itemize}
Selected approaches differ in how they consider all of these perspectives and are thus compared and described in Section~\ref{concepts} and Section~\ref{sec:analysis}.

\section{Conceptualization}
\label{concepts}

\subsection{General terms}
%A generalized architecture of data quality is depicted in Figure. 
There exist a number of discrepancies in the definition of many concepts, especially in data quality due to the contextual nature of quality~\cite{Batini:2006} .
Therefore, we first describe and formally define the research context terminology as well as each of the identified dimensions in more detail.

\textbf{RDF Dataset.}
In this document, we understand a data source as an access point for Linked Data in the Web. 
It provides a dataset and it may support multiple methods of access.
The RDF triples, RDF graph and the RDF datasets have been adopted by the W3C Data Access Working Group \cite{Beckett:2004,Hayes:2004,Brickley-2004}
Given an infinite set $\mathcal{U}$ of URIs (resource identifiers), an infinite set $\mathcal{B}$ of blank nodes, and an infinite set $\mathcal{L}$ of literals, a triple $ \langle s, p, o \rangle \in (\mathcal{U} \cup \mathcal{B})\times \mathcal{U} \times (\mathcal{U} \cup \mathcal{B} \cup \mathcal{L})$ is called an RDF triple; $s$, $p$, $o$ are called, respectively, the subject, the predicate and the object of the triple. An RDF graph $G$ is a set of RDF triples. A named graph is a pair  $\langle G,u \rangle$, where $G$ is called the default graph and $u\in\mathcal{U}$. An RDF dataset is a set of default and named graphs = $\lbrace G, (u_1,G_1), (u_2,G_2), ...(u_n,G_n)\rbrace$. 

\textbf{Data Quality.}
%WIQA
The concept of data quality is a domain-specific subconcept of the general concept of quality. 
A popular definition for quality is the "fitness for use"~\cite{qdefn}.
Data quality is commonly conceived as a multidimensional construct, as the "fitness for use" may depend on various factors such as accuracy, timeliness, completeness, relevancy, objectivity, believability, understandability, consistency, conciseness, availability, and verifiability~\cite{qconsumers}.
In terms of the semantic web, there are varying concepts of data quality.
The semantic metadata, for example, is an important concept to be considered when assessing the quality of datasets~\cite{Leigold}.
On the other hand, the notion of link quality is another important aspect in Linked Data that is introduced, where it is automatically detected whether a link is useful or not~\cite{Gueret}.
Also, it is to be noted that \textit{data} and \textit{information} are interchangeably used in the literature. 

\textbf{Data Quality Problems.}
%What is data quality problem? How the others have defined it?
The data quality problem refers to a set of issues that can affect the potentiality of the applications that use data. 
In the literature there is no such a specific definition related to data quality problems. 
In \cite{Flemming} the author does not provide a definition of it but implicitly explains that in terms of \textit{data diversity}. 
In \cite{Hogan} the authors discuss about \textit{errors} or \textit{noise} or \textit{difficulties} and in \cite{Hogan:2012} the author discuss about \textit{modelling issues} which are prone of the non exploitations of those data from the applications.

Bizer et al. \cite{Bizer} defines the data quality problems as a choice of the web-based information systems design which integrate information from different providers.  In \cite{Mendes} the problem of data quality is related to values being in conflict between different data sources as as a consequence of the diversity of the data. 
%
%%because it may be incomplete, poorly formatted, inconsistent,
%%What kind of problems we could have?
%%Errors which compromise the effectiveness of applications leveraging the resulting data. 
%%- Publishing errors; Incomplete; Incoherent; Poorly formatted; Inconsistent; Hijack; Dereferancability; Syntax errors; Link quality; Outdated; Incorrectness; Serialization problems; Inusability; System Problems; Inaccurate; Misleading; Outdated\\
%
%%Semantic Metadata
%%Data sources my have low quality such as misspelling, erroneous statements, etc; problems could be derived from the heterogeneity of data sources such as inconsistencies and duplicated entries; or be introduces by the tools employed. Changes to the data sources or to the underlying ontologies could also bring problems. 
%%data are modelled in a manner that is not
%%facilitative to generic consumption
%

\textbf{Data Quality Dimensions and Criteria.}
Data quality assessment involves the measurement of quality \textit{dimensions} or \textit{criteria} that are relevant to the consumer.
A data quality assessment \textit{metric} or \textit{measure} is a procedure for measuring an information quality dimension~\cite{Bizer}. 
These metrics are heuristics that are designed to fit a specific assessment situation~\cite{metric}.
Since the criteria are rather abstract concepts, the assessment metrics rely on quality \textit{indicators} that allow for the assessment of the quality of a data source w.r.t the criteria~\cite{Flemming}.
An assessment score is computed from these indicators using a scoring function. 

In \cite{Bizer}, the data quality dimensions are classified into three categories according to the type of information that is used as quality indicator: (1) Content Based - information content itself; (2) Context Based - information about the context in which information was claimed; (3) Rating Based - based on the ratings about the data itself or the information provider. 
However, we identify further dimensions and thus further categories to classify the dimensions, namely (1) Contextual; (2) Intrinsic; (3) Representation; (4) Accessibility and (5) Dataset Dynamicity, as depicted in Figure~\ref{fig:dimrel}.

\textbf{Data Quality Assessment Method.}
%WIQA
A data quality assessment methodology is defined as the process of evaluation if a piece of data meets in the information consumers need in a specific use case~\cite{Bizer}.
The process involves measuring the quality dimensions that are relevant to the user and comparing the assessment results with the users quality requirements.
%The steps involved in data quality assessment are: (1) formulating a research question; (2) selecting datasets and performing analyses; (3) detecting quality problems; (4) performing data quality analysis; (5) improving data quality and identifying short comings.
%%Semantic Metadata
%%There are three major steps involved in the evaluation procedure, including: setting up the evaluation context, detecting quality problems and calculating and analyzing the quality status. 

\begin{figure}
\includegraphics[scale=0.5]{Mindmap.pdf}
\caption{Conceptualization of the data quality domain.}
\label{fig:mindmap}
\end{figure}

\subsection{Linked Data quality dimensions}
% prior definitions
% our definition
%* example
%* relation to other quality dimensions
\subsubsection{Provenance}

\subsubsection{Consistency} 
The consistency dimension implies that two or more values do not conflict with each other \cite{Bizerthesis}. Similarly Flemming and Hogan define consistency in their respective works as an aspect that indicates contradictions in the data \cite{Flemming,Hogan}. Lei et al. determines consistency in terms of a problem known as deficiency, inconsistent representation, which denotes the situation where an instance is inconsistent with the ontologies \cite{Lei}. Mendes defines consistency of a dataset if it is free of conflicting information \cite{Mendes}. 

\subsubsection{Timeliness}
An important aspect of data is their update over time. The main time-related dimensions proposed in the literature is timeliness. However, there are two more dimensions related to time aspect such as currency and volatility. 
Bizer et al. defines timeliness as the degree to which information is up-to-date \cite{Bizerthesis} while Flemming define timeliness criterion as the currentness of the data provided by a source. It is part of the correctness of the data, as outdated data might meanwhile have become invalid \cite{Flemming}. The definition given in Mendes et al. corresponds to currency which is defined as the distance between the input date from the provenance graph to the current date. The recent data receives scores closer to 1 \cite{Mendes}. Similarly Rula et al. describes currency defined as a measure of the age of data and provides two types of currency based on document and graph level  \cite{Rula}. 

\subsubsection{Accuracy}
Bizer et al. defined accuracy as the degree of correctness and precision with which information in an information system represents states of the real world \cite{Bizerthesis}. There is no other work defining accuracy. Therefore, Lei et al. describe inaccurate annotation such as \textit{inaccurate labeling} and \textit{inaccurate classification} \cite{Lei}. Inaccurate labeling is when the mapping from the instance to the object is correct but is not properly labeled. For example, the person instance Trevor Collins might be correctly identified but wrongly marked as Both Trevor Collins. Inaccurate classification is when the knowledge of the source object has been correctly identified by not accurately classified. For example, the person Enrico Motta is classified as an instance of the high level class Person rather than the more precise class Professor. An additional problem identified is spurious annotation where there is no object to be mapped back to for an instance. B\"{o}hm et. al. describe the problem of detecting poor attributes i.e. those that do not contain useful values for data entries \cite{Bohm}. Guéret et. al. introduce the accuracy of interlinking between datasets by detecting whether there are open sameAs chains in the network \cite{Gueret}.


\subsubsection{Completeness}
Bizer defines completeness as the degree to which information is not missing. Schema completeness which is the degree to which entities and attributes are not missing in a schema; column completeness which is a function of the missing values in a column; and population completeness which refers to the ratio of entities represented in an information system to the complete population \cite{Bizerthesis}. 
In other works completeness is not properly defined but introduce problems related to it and the approach of how to identify if there is a problem. Therefore, in \cite{Hogan} completeness is defined as a problem of  incompleteness which includes equatable to a dead-link in the current HTML web, a software agent will not be able to retrieve data relevant to a particular task \cite{Hogan}. In \cite{Mendes} define completeness on the schema level and the data level. On the schema level, a data set is complete if it contains all of the attributes needed for a given task. On the data(instance) level, a data set is complete if it contains all of the necessary objects for a given task. 

\subsubsection{Amount of data}
Amount of Data is defined as the extent to which the volume of data is appropriate
for the task at hand \cite{Bizerthesis}.
An alternative definition says that the amount of data provided by a data source influences its usability \cite{Flemming} and is appropriate to approximate a true scenario precisely without getting false negative 	\cite{Chen}. It can be observed that there is a substantial agreement on the abstract definition of the amount of data. Although a main advantage of the Web of Data compared to the traditional web is the possibility to aggregate data from several sources, the necessity to match the underlying vocabularies puts that advantage into perspective. 

\subsubsection{Availability}
Availability is defined as the extent to which information is available, or easily and quickly retrievable  \cite{Bizerthesis} and refers to the proper functioning of all access methods \cite{Flemming}. 

\subsubsection{Understandibility}
Understandability is defined as the extent to which data is easily comprehended by the information consumer \cite{Bizerthesis}. Similarly Flemming relates understandibility to the comprehensibility of data i.e. the ease with which human consumers can understand and utilize the data \cite{Flemming}.

\subsubsection{Relevancy}
Relevancy is defined as the extent to which information is applicable and helpful for the task at hand  \cite{Bizerthesis}. 

\subsubsection{Reputation}
Since there is no definition for reputation we provide one which applies in the Web of Data. Reputation can be associated with a data publisher, a person, organisation, group of people, community of practice.
The data publisher should be identifiable for a certain (part of a) dataset. Reputation is usually a score, for example, a real value between 0 and 1. There are different possibilities to determine reputation and can be classified into direct/indirect approaches. Direct - survey in a community questioned about other members.
Indirect - use links/references/page rank.
\subsubsection{Verifiability}
Verifiability is defined as the degree and ease with which the information can be checked for correctness  \cite{Bizerthesis}. Similarly Flemming refers to verifiability criterion as to the means a consumer is provided with, which can be used to examine the data for correctness. Without such means, the only way of having a certain assurance of the correctness of the data is the consumer's trust in the source  \cite{Flemming}. 

\subsubsection{Interpretability}
Interpretability is defined as the extent to which information is in appropriate languages, symbols, and units, and the definitions are clear \cite{Bizerthesis}. Issues related to meaning (part of semantic), e.g. currency in dollars.  %and in jens 

\subsubsection{Rep.Conciseness}
In \cite{Bizerthesis}, representational conciseness is defined as "the extent to which information is compactly represented." 
This dimension is related to the format or representation of the data.
%Bizer mentions that XHTML, XML and RDF do not emphasise conciseness but are rather verbose in order to be self-descriptive and unambiguous.

For example, in \cite{Hogan:2012}, they identify the problem of very long URIs such as those that contain query parameters as an issue related to the representational conciseness. 
Keeping URIs short and human readable is highly recommended for large scale and/or frequent processing of RDF data as well as for efficient indexing and serialisation. 
Representing data, especially Linked Data, in a concise format not only contributes to the human readability of the data but also influences the performance of data when queried. 

\subsubsection{Rep.Consistency}
In \cite{Bizerthesis}, representational consistency is defined as "the extent to which information is represented in the same format".
This dimension is mainly related to the format or representation of the data.
However, representational consistency should not be considered similar to the consistency dimension as consistency implies that "two or more values do not conflict with each other."~\cite{Bizerthesis}.
In fact, consistency is considered a sub-dimension of representational consistency and is itself marked as an intrinsic dimension. 

%We define representational consistency as: \\
%\emph{the extent to which information is represented in the same format.} 
%and is compatible with previous data formats.
%http://semwebquality.org/mediawiki/index.php?title=Data_Quality#cite_note-7
For example, in \cite{Hogan:2012}, they report the (re-)use of declarative, extensible, shared vocabularies across the web as an important aspect contributing towards representational consistency.  
As rightly stated in \cite{Hogan:2012}, "Re-using well-known terms to describe resources in a uniform manner increases the interoperability of data published in this manner."
Re-use of well known vocabularies not only ensures that the data is consistently represented in different datasets but also supports data integration and management tasks. 
For example, when a data provider needs to describe information about people, FOAF should be the vocabulary of choice.

\subsubsection{Licensing}
In \cite{Flemming}, it is mentioned that "without a licence, the data consumer cannot legally use the data as intended." 
Additionally, the importance of the existence of a machine-readable (by including it in a voiD description) as well as a human-readable indication of a licence is emphasized.
Also, in \cite{Hogan:2012}, it is mentioned that "in order to enable information consumers to use your data under clear legal terms, each RDF document should contain a license under which the content can be used."
Licensing is part of the meta-information and provenance of a dataset.
Providing licensing information increases the usability of the dataset as the consumers are thus made aware of the legal rights and permissiveness under which the pertinent data are made available.

An empirical analysis done in \cite{Hogan:2012} showed that 27 PLDs (Pay Level Domains) i.e. 14.4\% returned some licensing information for some local document.
They found that, on average, providers gave licensing information for 3.4\% of local documents.
They also identified the need for an agreed-upon licensing property and an agreed set of common license URIs 

\subsubsection{Performance}
In \cite{Flemming}, performance is denoted as that which "comprises aspects of enhancing the performance of a source as well as measuring of the actual values." 
However, in \cite{Hogan:2012}, performance is associated with issues such as avoiding prolix RDF features such as (i) RDF reification, (ii) RDF containers and (iii) RDF collections. 
These should be avoided as they are cumbersome to represent in triples and can prove to be expensive to support in performance or data intensive environments.
Performance is related to the accessibility of a dataset.

For example, \cite{Hogan:2012} identifies the hosting of unstable URIs as a problem associated with the performance of agents and applications. 
The problem could occur when remote resources are removed, changed, updated or moved and the links are not updated and thus affects the performance of a dataset. 

\subsubsection{Objectivity} 
In \cite{Bizerthesis}, objectivity is expressed as "the extent to which information is unbiased, unprejudiced and impartial."
Objectivity is an intrinsic dimension. 
It highly depends on the type of information and overlaps with the concept of accuracy.

For example, the height of a building can be measured objectively whereas the objectivity of product descriptions require the preference of the information provider.

\subsubsection{Believability}
In \cite{Bizerthesis}, believability is explained as "the extent to which information is regarded as true and credible" and can be seen as expected accuracy.
Believability is a subjective dimension and is part of the provenance of a dataset.
%Example

\subsubsection{Response Time}
In \cite{Bizerthesis}, response time is defined that which "measures the delay between submission of a request by the user and reception of the response from the system."
This dimension depends on the type and complexity of the request and is related to the accessibility of a dataset.

Low response time hinders the usability as well as accessibility of a dataset.
Locally replicating or caching information are possible means of improving the response time.

\subsubsection{Security}
%no definition found
%The homepage of a company has been inadequately protected. Hackers alter its content because the websiteÕs information is not secure. - Eppler
%reliable and secure infrastructure
%# of weak logins

\subsubsection{Uniformity}
\cite{Flemming} refers to uniformity as "the usage of established techniques in order to increase the usability of the data.", similar to the representational consistency dimension.
Uniformity is part of the representation of a dataset as it recommends the usage of established formats and vocabularies, referencing URIs of established datasets and stating the content-types as specifically as possible.
The presence of uniformity in a dataset increases the usability of the data as it enables consumers to process the data easily, helps the user to better understand the semantics of the data and also increases the chances of connecting the dataset with others.

\subsubsection{Versatility} 
\cite{Flemming} refers to versatility as the "alternative representations of the data and its handling."
Versatility is part of the representation of a dataset.
%contradictory to rep.consistency

\subsubsection{Validity of documents}
"Validity of documents consists of two aspects influencing the usability of the documents: the valid usage of the underlying vocabularies and the valid syntax of the documents.", as specified in  \cite{Flemming}. 
This is an intrinsic dimension similar to the accuracy, consistency and objectivity dimensions.

RDF datasets with mistyped descriptions can result in an incorrect interpretation of the data.
Moreover, invalid usage of certain vocabularies can result in consumers not able to process data as intended.
Syntax errors, typos, not defining any additional properties that are added, use of deprecated classes and properties all add to the problem of the validity of the document.
Correct parsing of the triples should be an important aspect to be considered when publishing RDF data. 

\subsubsection{Conciseness}
 \cite{Mendes} characterizes conciseness as follows: "On the schema level, a dataset is concise if it does not contain redundant attributes (two equivalent attributes with different names). 
On the data (instance) level, a data set is concise if it does not contain redundant objects (two equivalent objects with different identifiers). 
The extensional conciseness measures the number of unique objects in relation to the overall number of object representations in the dataset.
Similarly, intensional conciseness measures the number of unique attributes of a dataset in relation to the overall number of attributes in a target schema."
%Conciseness can be measured as follows:
%\[\textit{Conciseness(p)} = \frac{\textit{obj. with uniq. values for p in dataset}}
%{\textit{all uniq. obj. with p in dataset}}\]
Conciseness is part of the representation of a dataset and implies that data should be compactly represented yet complete and to the point. 

For example, as shown in \cite{Mendes}, when two datasets use the same schema, it is an example of intensional conciseness.
On the other hand, when the two editions of DBpedia (from two different language) only contained one URI per object,it is an example of extensional conciseness.

\subsubsection{Coherence}


Figure|~\ref{fig:dimrel} shows the classification of the dimensions in several groups as well as the inter and intra relations between them.

\begin{figure*}[htb]
\includegraphics[width=5in]{DimensionsRelations.pdf}
\caption{Linked data quality dimensions and the relations between them.}
\label{fig:dimrel}
\end{figure*}
