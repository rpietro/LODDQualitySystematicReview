Two reviewers, from independent institutions, conducted the systematic review following the standard procedures as described in \cite{kitchenham:2004, Moher:2009}. A systematic review can be addressed for several reasons such as the summarisation and comparison in terms of advantages and disadvantages of the approaches applied on a same evidence, the identification of uncovered problems, or the generation of a new idea to cover the emphasized problems. Our systematic review was conducted as an hybrid of the first and second reason by creating first a set of rules used for arranging our work which were followed by the definition of a research question. Therefore, the research question we defined conducted us through two individual literature search.

%then compared results to agree to a final list of papers to be included in the study. Both conducted the qualitative and quantitative review of the articles.

\subsection{Verify whether a similar systematic review already exists}
In order to justify the need of conducting the systematic review, we first conducted a search to ensure that a similar study has not already been done. This ensures that the formulated research question was not already answered through previous answers. We did not come across any study focused on data quality assessment methodologies for Linked Data datasets. However, we came across a comprehensive review \ref{Batini:2006}, which reviews 12 methodologies for assessing the data quality of datasets available on the web in structured or semi-structured formats.

\subsection{Formulate a research question}
The goal of this review is to analyse existing methodologies for assessing the quality of structured data, with particular interest in Linked Data.
To achieve this goal, we aim to answer the following general research question: \\
\textit{What are the existing approaches/methodologies for assessing the quality of Linked Data datasets?}\\We can divide this general research question into small research question as in the following: \\
-\textit{What are the problems that each methodology wants to tackle?}\\
-\textit{What are the quality criteria supported by the proposed methodologies?}\\
-\textit{What are the assessment methods proposed by the different methodologies?}

\subsection{Define eligibility criteria}
The eligibility criteria is an important element of any systemtic review. First, each member created a set of inclusion and exclusion criteria on their own. Second, as a result of a discussion between both members a list of eligible criteria was obtained as follows:
\begin{itemize}
\item Inclusion criteria:
\begin{itemize}
\item Studies that were published in English between 2005 and 2012.
\item Studies that were focused on data quality assessment in Linked Data datasets
\item Studies that were focused on data quality assessment of structured data
\end{itemize}
\item Exclusion criteria:
\begin{itemize}
\item Studies that were focused on data quality management.
\item Studies that does not focus neither on Linked Data nor on structured data such as ontology assessment.
\item Studies that does non propose any methodology or framework about the assessment of quality in Linked Data
\item Studies that does not focus on quality assessment of Linked Data but only mentions the term e.g
\end{itemize}
\end{itemize}
\begin{figure*}[ht]

\includegraphics[width=6.5in]{Numberofarticles.pdf}
\caption{Number of articles retrieved during literature search.}
\label{fig:noofarticles}
\end{figure*}

\subsection{Build and run search strategy}
%Ex spreadsheet: http://goo.gl/3s8GV
It is necessary to build and follow a search strategy. Search strategies are usually iterative and are ran separately by both members. Therefore, we created two spreadsheets where we could store and keep trace of our search strategy. Based on the research question and the eligible criteria, first we identified several terms to be most
appropriate for the systematic review: \textit{data}, \textit{quality}, \textit{data quality}, \textit{assessment}, \textit{evaluation}, \textit{methodology}, \textit{improvement}, or \textit{linked data}, which are used as follows:
\begin{itemize}
\item \textit{linked data} and (\textit{quality} OR \textit{assessment} OR \textit{evaluation} OR \textit{methodology} OR \textit{improvement})
\item  \textit{data} OR \textit{quality} OR \textit{data quality} AND \textit{assessment} OR \textit{evaluation} OR \textit{methodology} OR \textit{improvement}
\end{itemize}
The next decision was to find the suitable field (i.e. title, abstract and full-text) to apply the search
string on. In our experience, searching in the \textit{title} alone does not always provide us with all relevant publications. Thus, \textit{abstract} or \textit{full-text} of publications should potentially be included. On the
other hand, since the search on the full-text of studies results in many irrelevant publications, we chose
to apply the search query additionally on the \textit{abstract} of the studies. This means a study is selected
as a candidate study if its \textit{title} or \textit{abstract} contains the keywords defined in the search string.

After we defined the search strategy, we chose the right databases for our research. The searches were done in the following list which include search engines and digital libraries:
\begin{itemize}
\item Google Scholar
\item ISI Web of Science
\item ACM Digital Library
\item IEEE Xplore Digital Library
\item Springer Link
\item Science Direct
\end{itemize}
List of journal:
\begin{itemize}
\item Semantic Web Journal
\item Journal of Web Semantics
\item Journal of Data and Information Quality
\item Journal of Data and Knowledge Engineering
\end{itemize}

Conferences and their Respective Workshops:
\begin{itemize}
\item International Semantic Web Conference (ISWC)
\item European Semantic Web Conference (ESWC)
\item Asian Semantic Web Conference (ASWC)
\item International World Wide Web Conference (WWW)
\item Semantic Web in Provenance Management (SWPM)
\item Consuming Linked Data (COLD)
\item Linked Data on the Web (LDOW)
\item Web Quality
\end{itemize}
For the journal list and the conferences we conducted a hand search to get more potential articles. First we looked at the table of contents of the key journals and conferences and second we extracted any article identified as a potential match based on eligibility criteria.
\subsection{Importing files to endnote}
The bibliographic metadata about each primary study were recorded using the bibliography management platform Mendeley\footnote{https://www.mendeley.com/}.

\subsection{Reviewing titles and abstracts}
Both reviewers independetly screen the titles and abstracts of the retrieved literature to identify the potentially eligible articles. After comparing the shortlisted potentially eligible articles based on the eligibility criteria then we finalized the list by mutual agreement. In case of disagreement we resolved the problem either by mutual consensus or we created a list of articles to go under a more detailed review. Thus, each of the reviewers read the title and the abstract again and also the full-text for getting more detailed information. Finally, we merged both manuscript lists from each reviewers by removing the duplicated records.

\subsection{Retrieve and analyze other potential articles}
The other strategy decisions was to apply further steps such as:
\begin{itemize}
\item Look at the reference in the selected papers.
\item Take the title of the paper and put it in Google Scholar and look at "Cited By" papers and retrieve the article if it is related to the eligible criteria.
\item Take each data quality criteria individually and perform a related article search.
\end{itemize}
from References of selected articles and then evaluate them for inclusion/exclusion criteria

\subsection{Abstract data for quantitative and qualitative analysis}

As shown in Figure \ref{fig:noofarticles}, we first applied the search query on each data source separately. To remove irrelevant studies from the result returned from the different data sources, we scanned the articles by title based on the eligibility criteria. We further imported the results obtained to Mendeley to remove duplicate studies. Subsequently, we reduced the number of studies to 182. Then, we read the abstract of each publication carefully and further we retrieve and analyse paper from the references and we obtained 68 number of studies. Finally, we compared  the shortlisted articles among reviewers by scanned the full-text of the publications. The result comprised 9 articles related to methodologies for assessing Linked Data which will be our final set of primary studies and 24 article related to provenance.

\textit{To analyze the information appropriately, we required a suitable qualitative data analysis method applicable to our dataset. We used coding as our qualitative analysis methods}

\subsection{List of selected papers}
%Out of the total number of articles retrieved from the initial literature survey, most were only related to the general aspects of data quality assessment of the data available on the Web. After refining our search strategy by using a combination of keywords and the advanced search forms available for most of the online databases, we retrieved (no.?) of articles. After reading through the articles in detail, only 9 were identified to be specifically those reporting a methodology or framework for data quality assessment of Linked Data. %Out of the 9 articles, (no.) are from the year (?), (conference/journal). Additionally, those articles related to provenance quality assessment, were also retrieved. A total number of (48?) articles were identified. 

The result of the above describe methodology is XX papers from 2010 to 2012 that are reported in Table XX and that will be the core of our surveys.

ADD TABLE
\begin{table*}[htb]
\caption{List of the selected papers.} 
\label{dimex}
\begin{tabular}{ | p{3cm} | p{0.5cm} | p{10cm} | }
\hline
\textbf{Citation} & \textbf{Year} & \textbf{Title} \\
\hline
Bizer et.al.,2009 & 2009 & Quality-driven information filtering using the WIQA policy framework \\	
\hline
B\"ohm et.al., 2010 & 2010  & Profiling linked open data with ProLOD \\	
\hline
Chen et.al., 2010 & 2010 & Hypothesis generation and data quality assessment through association mining \\
\hline
Flemming et.al., 2010 & 2010 & Assessing the quality of a Linked Data source \\
\hline
Gu\'eret, 2011 & 2011 & Linked Data Quality Assessment through Network Analysis \\
\hline
Hogan et.al., 2010 & 2010 & Weaving the Pedantic Web \\
\hline
Hogan et.al.,2012 & 2012 & An empirical survey of Linked Data conformance \\
\hline
Lei et.al., 2007 & 2007 & A framework for evaluating semantic metadata \\
\hline
Mendes et.al.,2012 & 2012 & Sieve: Linked Data Quality Assessment and Fusion \\
\hline
Mostafavi et.al., 2004 & 2004 & An ontology-based method for quality assessment of spatial data bases	\\
\hline
Hartig, 2009 & 2009 & Trustworthiness of Data on the Web \\
\hline
Hartig et.al., 2009 & 2009 & Using Web Data Provenance for Quality Assessment \\
\hline
Gamble et.al., 2011 & 2011 & Quality, Trust, and Utility of Scientific Data on the Web: Towards a Joint Model \\
\hline
Shekarpour et.al., 2008 & 2008 & Modeling and evaluation of trust with an extension in semantic web \\
\hline
Golbeck et.al., 2006 & 2006 & Using Trust and Provenance for Content Filtering on the Semantic Web \\
\hline
Gil et.al., 2002 & 2002 & Trusting Information Sources One Citizen at a Time \\
\hline
Golbeck et. al., 2003 & 2003 & Trust Networks on the Semantic Web \\
\hline
Gil et.al., 2007 & 2007 & Towards content trust of web resources \\
\hline
Jacobi et.al., 2011 & 2011 & Rule-Based Trust Assessment on the Semantic Web \\
\hline
Bonatti et. al., 2012 & 2012 & Robust and scalable linked data reasoning incorporating provenance and trust annotations \\
\hline
Maurino et.al., 2012 & 2012 & Capturing the Age of Linked Open Data: Towards a Dataset-independent Framework \\
\hline
\end{tabular}
\end{table*}

\subsection{Comparison perspective of selected papers}
There exist several perspectives that can be used to analyze and compare selected papers:
\begin{enumerate}
\item the phases/steps that compose the assessment analysis
\item the dimensions that are chosen in the papers to assess data quality levels;
\item the metrics associated to dimensions
\item the types of data that are considered in papers;
\item the level of automatization of supported tools
\end{enumerate}

Selected papers differ in how they consider all of these perspectives.
Assessment activity measures the quality of data collections along relevant quality dimensions. The steps of the assessment are:
\subsubsection{Data quality assessment steps}

Table~\ref{metricsteps} comprises of the steps involved in the data quality assessment process. 

Assessment steps: related to the common steps of each work used to assess the quality in LOD. For each step we provide input and output, a supported tool and the involvement of the user in the tool usage.  
The "Assessment steps" measures the quality of data collections along relevant quality dimensions; in particular the measurement of quality dimensions is provided by a set of metrics defined for each dimension. From the selected paper we identify the following sequence of activities customized by  different data quality methods:
\begin{itemize}
\item Requirements analysis (optional): The multidimensionality of the information quality makes it dependent on a number of factors that can be achieved by the analyses of the user requirements. The requirement analysis is optional since it is not always provided from the methods provided in LOD papers.

\item Data Quality Checklist: estimates only those metrics for which we may answer yes or no. 

\item Statistics and low-level analysis: provides some generic statistics on the dataset based on some heuristics.

\item Aggregated and higher level metrics: in this category we include all the metrics that are not included in the Data Quality Checklist. Furthermore, in this step the single metrics or the combination of them produce a value within a range [0;1]. 

\item Comparison (optional): used when the resulted measurements provided in step "Aggregated and higher level metrics" are compared to reference values such as previous values from dataset in the same domain or gold standard values, in order to enable a diagnosis of quality. 

\item Interpretation: gives an interpretation to the results obtained from step Data Quality Checklist or Aggregated and higher level metrics.
\end{itemize}

The definition of \emph{the qualities dimensions, and metrics} to assess
data is a critical activity. In general, multiple metrics can be associated with each
quality dimension. In some cases, the metric is unique and the theoretical definition of
a dimension coincides with the operational definition of the corresponding metric.
Due to the that the linked open data paradigm is the fusion of three different research areas namely \emph{semantic web } (for the capability to generate semantic connections among data),  \emph{world wide web} (related to the availability and open access to such data) and \emph{data management} (thanks to the fact that there is the need to mange large set of heterogeneous and distributed data), selected papers uses quality dimension taken for the specific area. thus for example in the data management literature provides a thorough classification of data quality dimensions.  The six most important classifications of quality
dimensions are provided by Wand and Wang [1996]; Wang and Strong [1996]; Redman
[1996]; Jarke et al. [1995]; Bovee et al. [2001]; and Naumann [2002]. By analyzing these
classifications, it is possible to define a basic set of data quality dimensions, including
accuracy, completeness, consistency, and timeliness, which constitute the focus of the
majority of authors [Catarci and Scannapieco 2002].
However, no general agreement exists either on which set of dimensions defines the
quality of data, or on the exact meaning of each dimension, and also in the lod field we find the same problem
Concerning the web field the most important dimensions are the provenance \cite{}, trust\cite{}, ADD OTHER

Concerning semantic web, the most important dimensions are Provenance, Consistency, Timeliness, Accuracy, 
Completeness, Amount of Data, Availability (accessibility), Understandability (Comprehensibility), Relevancy, Reputation, Verifiability, Interpretability, Rep. Conciseness, Rep. Consistency, Licencing, Performance, Objectivity, Believability, Response Time, Security, Uniformity, Versatility, Validity of documents, Conciseness, Coherence (linking - internal/external) and Trust.

The ultimate goal of an assessment activity is the analysis of data that, in general, describe
real world objects in a format that can be stored, retrieved, and processed by a software
procedure, and communicated through a network. In the field of lod, most
authors either implicitly or explicitly distinguish three types of data:
\begin{itemize}
\item RDF triple. Given an infinite set $\mathcal{U}$ of URIs (resource identifiers), an infinite set $\mathcal{B}$ of blank nodes, and an infinite set $\mathcal{L}$ of literals, a triple $ \langle s, p, o \rangle \in (\mathcal{U} \cup \mathcal{B})\times \mathcal{U} \times (\mathcal{U} \cup \mathcal{B} \cup \mathcal{L})$ is called an RDF triple; $s$, $p$, $o$ are called, respectively, the subject, the predicate and the object of the triple.
\item Graph. An RDF graph $G$ is a set of RDF triples. A named graph is a pair  $\langle G,u \rangle$, where $G$ is called the default graph and $u\in\mathcal{U}$. [ANDREA we need to underline that a graph is a set of datasource provided by different providers]
\item Dataset. An RDF dataset is a set of default and named graphs = $\lbrace G, (u_1,G_1), (u_2,G_2), ...(u_n,G_n)\rbrace$.
\end{itemize}

A set of software tools are needed for supporting the assessment phase. such tools implements the methodologies and metrics defined in the above described steps. Due to the nature of the quality dimensions and related metrics it is possible that some activities are fully or semi automatically or manually realized. 